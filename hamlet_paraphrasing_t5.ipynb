{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2749f694-e0b2-483f-81dd-d88f189bfe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd # Handles tabular data (DataFrames) and CSV reading\n",
    "import numpy as np # Provides numerical and array operations\n",
    "import matplotlib.pyplot as plt # Creates visualizations\n",
    "import seaborn as sns # Enhances visualizations with attractive statistical plots\n",
    "import nltk # Provides NLP tools like tokenization, stopwords, and text processing\n",
    "from nltk.corpus import stopwords # Contains common stopwords (e.g., \"the\", \"and\", \"is\") to remove from text\n",
    "from nltk.tokenize import word_tokenize # Splits text into individual words (tokens)\n",
    "from wordcloud import WordCloud  # Creates word clouds from text data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "435dafd7-707f-4f4a-9793-9f8aba84fde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Hamlet dataset\n",
    "hamlet_url = 'https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-17/hamlet.csv'\n",
    "df_hamlet = pd.read_csv(hamlet_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ed43a06-0213-4045-a70d-347e4ca280da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your max_length is set to 18, but your input_length is only 17. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " FRANCISCO at his post. Enter to him BERNARDO\n",
      "\n",
      "Summarized Text:\n",
      " The Daily Discussion is open to everyone. Please share your thoughts on this week\n"
     ]
    }
   ],
   "source": [
    "import os # Provides a way to interact with the operating system, used here to set environment variables\n",
    "from transformers import pipeline # Loads pre-trained NLP models for summarization and paraphrasing \n",
    "\n",
    "# Fix Hugging Face symlink warning for Windows\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "# Load a summarization model explicitly\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Get first dialogue text\n",
    "dialogue_text = df_hamlet['dialogue'].iloc[0]\n",
    "\n",
    "# Dynamically adjust max_length to avoid warnings\n",
    "max_length = min(len(dialogue_text.split()) + 10, 50)\n",
    "\n",
    "# Generate summary\n",
    "summary = summarizer(dialogue_text, max_length=max_length, min_length=10, do_sample=False)\n",
    "\n",
    "# Output results\n",
    "print(\"Original Text:\\n\", dialogue_text)\n",
    "print(\"\\nSummarized Text:\\n\", summary[0]['summary_text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aa396fc-3b18-4fd1-acd7-c43b66437a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Paraphrased Text:\n",
      " : The scene opens with FRANCISCO at his post. Then BERNARDO enters.\n"
     ]
    }
   ],
   "source": [
    "#Evaluate if the T5 model could successfully reword structured inputs with using prompts \n",
    "from transformers import pipeline # Loads pre-trained NLP models for summarization and paraphrasing \n",
    "\n",
    "paraphraser = pipeline(\"text2text-generation\", model=\"t5-base\") #Load the t5-base model as a text-to-text generation pipeline.\n",
    "\n",
    "# Reformat input to sound more like a sentence\n",
    "dialogue_text = \"The scene opens with FRANCISCO at his post. Then BERNARDO enters.\"\n",
    "\n",
    "prompt = f\"Paraphrase this: {dialogue_text}\" # Create a prompt \n",
    "response = paraphraser(prompt, max_length=50, min_length=10, do_sample=False) #Send the prompt to the model for paraphrasing.\n",
    "\n",
    "print(\"\\nParaphrased Text:\\n\", response[0]['generated_text']) #Print the paraphrased text from the response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c45593c3-866a-49d3-920d-15990d5588c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare Data: Labeled Examples\n",
    "from datasets import Dataset #Loads the Dataset class from Hugging Faceâ€™s datasets library.\n",
    "#Dataset class is used to create and manage datasets for training language models.\n",
    "\n",
    "# Create labeled examples\n",
    "# Simulates a mini supervised dataset: each example teaches the model how to rewrite a sentence.\n",
    "train_data = [\n",
    "    {\"input\": \"paraphrase: FRANCISCO at his post. Enter to him BERNARDO\", \n",
    "     \"output\": \"The scene opens with FRANCISCO at his post. Then BERNARDO enters.\"},\n",
    "    {\"input\": \"paraphrase: Nay, answer me: stand, and unfold yourself.\", \n",
    "     \"output\": \"Please answer and identify yourself.\"},\n",
    "    {\"input\": \"paraphrase: Long live the king!\", \n",
    "     \"output\": \"May the king live long!\"},\n",
    "    {\"input\": \"paraphrase: Bernardo?\", \n",
    "     \"output\": \"Is that you, Bernardo?\"},\n",
    "    {\"input\": \"paraphrase: You come most carefully upon your hour.\", \n",
    "     \"output\": \"You arrived exactly on time.\"}\n",
    "]\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_list(train_data) #Converts the list into a Hugging Face Dataset object so it can be tokenized, batched, and used for training.\n",
    "dataset = dataset.train_test_split(test_size=0.2) #Split the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b2c4433-1188-4dec-962f-6327ba08ddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Model & Tokenizer\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model_name = \"t5-base\"  \n",
    "#Breaks down text into tokens that the model can understand and turns predictions back into text.\n",
    "#SentencePiece tokenizer, which splits text into subwords. It treats input as a raw stream of characters, making it flexible for many languages. \n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name) \n",
    "#T5 model\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f973c14-eb93-4b6b-9962-ea7b0e3b5e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the Dataset\n",
    "#Set the maximum number of tokens allowed in the input and output sentences.\n",
    "#Ensures all sequences have the same length, which is important for model training.\n",
    "max_input_length = 64\n",
    "max_target_length = 64\n",
    "\n",
    "def preprocess(example):\n",
    "    inputs = tokenizer(example[\"input\"], max_length=max_input_length, padding=\"max_length\", truncation=True)\n",
    "    targets = tokenizer(example[\"output\"], max_length=max_target_length, padding=\"max_length\", truncation=True)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True) #Applies the preprocess function to every item in the dataset. \n",
    "#batched=True lets it process multiple examples at once (faster).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8001c345-cd2a-4584-b9f7-3c85a9207afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mosey\\anaconda3\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\mosey\\AppData\\Local\\Temp\\ipykernel_19580\\1578526564.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 01:10, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>16.360489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>13.677347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>12.208059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>11.193885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>12.081200</td>\n",
       "      <td>10.306516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>12.081200</td>\n",
       "      <td>9.548788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>12.081200</td>\n",
       "      <td>8.964210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>12.081200</td>\n",
       "      <td>8.559940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>12.081200</td>\n",
       "      <td>8.309845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.876300</td>\n",
       "      <td>8.202006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paraphrase: FRANCISCO at his post. Enter to him BERNARDO at his post.\n"
     ]
    }
   ],
   "source": [
    "#Fine-Tune the Model\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Define training arguments to define how the model should be trained\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5-paraphrase\",\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    learning_rate=5e-5, # How fast model updates weights\n",
    "    per_device_train_batch_size=2, \n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=10, \n",
    "    weight_decay=0.01, #regularization technique used during training to prevent overfitting\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Train the model. Now the T5 model is being fine-tuned on paraphrased examples. The model is learning how to reword.\n",
    "trainer.train()\n",
    "\n",
    "# Paraphrasing function (after training is done)\n",
    "#This function takes new input, uses fine-tuned model to generate a paraphrase, and returns it.\n",
    "def paraphrase_text(text):\n",
    "    input_text = f\"paraphrase: {text}\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "    output_ids = model.generate(input_ids, max_length=64, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True) \n",
    "\n",
    "# Test the paraphrasing after training\n",
    "print(paraphrase_text(\"FRANCISCO at his post. Enter to him BERNARDO\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab83630-4679-4925-b36f-e081fb1e9b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
